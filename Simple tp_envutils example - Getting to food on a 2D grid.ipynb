{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating how to use tp_envutils on a simple grid example\n",
    "\n",
    "## The environment:\n",
    "- A 2D grid where the player can move up/down/left/right\n",
    "- One block is food, one block is an enemy\n",
    "- Eating the food gives positive reward, hitting the enemy gives a penalty, and movement penalty is -1 to encourage going straight for the food\n",
    "\n",
    "## The RL - genetic evolution\n",
    "1. 100 agents run through 10 episodes of this environment\n",
    "2. At the end of each generation, the 10 best are kept\n",
    "3. The remaining 90 are replaced by copies of the 10 (probability distribution based on how well the 10 did relative to each other), and mutated slightly\n",
    "\n",
    "## Scaling the environment\n",
    "In this example, the environment is first set to a 4x4 grid, and after 15 generations, increased to 10x10. This is done to speed up learning, as starting out on a 10x10 grid takes longer. The principle still holds though, so after increasing the size and running for 10 more generations, the agents perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tp_envutils import Env, Block, Agent, PopulationTakeTop\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.constraints import min_max_norm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X,Y = 4,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp_layer = Input(shape=(4,), dtype=np.float32)\n",
    "    hidden = Dense(8, activation=\"tanh\", kernel_constraint=min_max_norm(-1.,1.))(inp_layer)\n",
    "    out = Dense(4, activation=\"softmax\", kernel_constraint=min_max_norm(-1.,1.))(hidden)\n",
    "    model = Model(inp_layer,out)\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the methods for the Env constructor\n",
    "The whole point of the Env class is to get rid of boilerplate code, and provide hooks for all the parts where your learning process varies.\n",
    "\n",
    "Here we provide only what's unique to this particular environment / agent / learning approach configuration\n",
    "\n",
    "For example, if you wanted to use DQN, you could include updating your Q values in the Env, either in take_action, or defining call_every and doing it there\n",
    "\n",
    "**To check which methods / parameters you have available, you can call the Env.help() static method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(state,action):\n",
    "    oldx,oldy = state['blocks'][0].x,state['blocks'][0].y\n",
    "    state['blocks'][0].action(action)\n",
    "    newx,newy = state['blocks'][0].x,state['blocks'][0].y\n",
    "\n",
    "    state['obs'] += [oldx-newx,oldy-newy,oldx-newx,oldy-newy]\n",
    "\n",
    "    if np.count_nonzero(state['obs'][0,:2]) == 0:\n",
    "        return state,10,True\n",
    "    elif np.count_nonzero(state['obs'][0,2:]) == 0:\n",
    "        return state,-10,True\n",
    "    else:\n",
    "        return state,-1,False\n",
    "\n",
    "def get_observation(state):\n",
    "    return state['obs']\n",
    "    \n",
    "def get_start_state():\n",
    "    p,e,f = (Block(X,Y),Block(X,Y),Block(X,Y))\n",
    "    while (e.x,e.y) == (f.x,f.y):\n",
    "        e = Block(X,Y)\n",
    "    while (p.x,p.y) == (f.x,f.y) or (p.x,p.y) == (e.x,e.y):\n",
    "        p = Block(X,Y)\n",
    "    return {'blocks': (p,e,f), 'obs':  np.asarray([(*(f-p), *(e-p))],dtype=np.float)}\n",
    "\n",
    "def display_env(state):\n",
    "    env = np.zeros((X,Y,3), dtype=np.uint8)\n",
    "    env[state['blocks'][1].y,state['blocks'][1].x] = (0,0,255)\n",
    "    env[state['blocks'][2].y,state['blocks'][2].x] = (0,255,0)\n",
    "    env[state['blocks'][0].y,state['blocks'][0].x] = (255,0,0)\n",
    "    return env\n",
    "\n",
    "env = Env(\n",
    "    take_action=take_action,\n",
    "    get_action=None, # provided in the Agent class\n",
    "    get_start_state=get_start_state,\n",
    "    display_env=display_env,\n",
    "    get_observation=get_observation,\n",
    "    steps_per_ep=30,\n",
    "    stat_every=10,\n",
    "    printing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:07<00:00, 13.65agents created/s]\n"
     ]
    }
   ],
   "source": [
    "TOTAL_AGENTS = 100\n",
    "EPISODES_PER_GENERATION = 10\n",
    "\n",
    "agents = []\n",
    "for i in tqdm(range(TOTAL_AGENTS), ascii=True, unit='agents created'):\n",
    "    agents.append(Agent(get_model(),env))\n",
    "    \n",
    "pop = PopulationTakeTop(agents,\n",
    "                        ep_per_gen=EPISODES_PER_GENERATION,\n",
    "                        take_top=10,\n",
    "                        mutation_chance=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First running on the 4x4 grid for 15 generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [01:18<00:00,  1.20agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1, best results:\n",
      "{'rewards': [-13, -30, 10, -11, 10, 9, -30, 10, -30, -30], 'sumrewards': -105, 'aggr': {'avg': [-10.5], 'min': [-30], 'max': [10]}}\n",
      "Generation 1, averages of top 10 agents:\n",
      "[[-10.5], [-12.2], [-12.3], [-12.4], [-12.6], [-14.2], [-14.4], [-14.4], [-14.5], [-14.6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:13<00:00,  6.95agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2, best results:\n",
      "{'rewards': [-10, -30, -30, 9, 9, -11, 9, 10, 9, -30], 'sumrewards': -65, 'aggr': {'avg': [-6.5], 'min': [-30], 'max': [10]}}\n",
      "Generation 2, averages of top 10 agents:\n",
      "[[-6.5], [-6.7], [-6.9], [-10.5], [-12.1], [-12.3], [-12.3], [-12.5], [-12.6], [-12.7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:12<00:00,  7.79agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3, best results:\n",
      "{'rewards': [-10, 10, 9, 10, -30, -11, 9, 10, 10, 7], 'sumrewards': 14, 'aggr': {'avg': [1.4], 'min': [-30], 'max': [10]}}\n",
      "Generation 3, averages of top 10 agents:\n",
      "[[1.4], [-1.3], [-6.5], [-6.7], [-8.1], [-8.2], [-8.7], [-10.3], [-10.4], [-10.4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:11<00:00,  8.01agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4, best results:\n",
      "{'rewards': [9, 10, 9, -30, -30, 9, 9, -30, 10, 10], 'sumrewards': -24, 'aggr': {'avg': [-2.4], 'min': [-30], 'max': [10]}}\n",
      "Generation 4, averages of top 10 agents:\n",
      "[[-2.4], [-4.6], [-4.6], [-5.7], [-6.6], [-8.4], [-8.5], [-8.7], [-9.1], [-10.2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:12<00:00,  6.44agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5, best results:\n",
      "{'rewards': [10, 9, -30, -30, 9, 10, 9, -30, 9, 9], 'sumrewards': -25, 'aggr': {'avg': [-2.5], 'min': [-30], 'max': [10]}}\n",
      "Generation 5, averages of top 10 agents:\n",
      "[[-2.5], [-2.9], [-2.9], [-5.5], [-6.4], [-6.7], [-8.2], [-8.3], [-8.5], [-8.7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:10<00:00,  7.41agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6, best results:\n",
      "{'rewards': [9, 9, 7, 10, 8, 9, 7, -30, 9, -30], 'sumrewards': 8, 'aggr': {'avg': [0.8], 'min': [-30], 'max': [10]}}\n",
      "Generation 6, averages of top 10 agents:\n",
      "[[0.8], [0.6], [-1.3], [-2.6], [-3.5], [-4.4], [-4.5], [-5.2], [-5.4], [-5.6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:10<00:00,  9.68agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7, best results:\n",
      "{'rewards': [10, 9, 9, -11, -10, 7, 7, -30, 10, 9], 'sumrewards': 10, 'aggr': {'avg': [1.0], 'min': [-30], 'max': [10]}}\n",
      "Generation 7, averages of top 10 agents:\n",
      "[[1.0], [-0.9], [-1.3], [-2.7], [-2.7], [-2.8], [-4.4], [-4.9], [-5.0], [-6.6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:10<00:00, 10.08agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 8, best results:\n",
      "{'rewards': [7, 9, 10, 8, 7, 8, 6, -30, 9, 9], 'sumrewards': 43, 'aggr': {'avg': [4.3], 'min': [-30], 'max': [10]}}\n",
      "Generation 8, averages of top 10 agents:\n",
      "[[4.3], [1.0], [0.9], [0.3], [-0.9], [-1.0], [-3.0], [-3.0], [-3.1], [-3.1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:09<00:00, 10.41agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 9, best results:\n",
      "{'rewards': [9, 9, 5, 8, 6, -10, 7, 9, 9, 10], 'sumrewards': 62, 'aggr': {'avg': [6.2], 'min': [-10], 'max': [10]}}\n",
      "Generation 9, averages of top 10 agents:\n",
      "[[6.2], [5.2], [2.7], [0.7], [0.6], [-0.5], [-0.8], [-1.2], [-1.4], [-1.6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:09<00:00, 10.19agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 10, best results:\n",
      "{'rewards': [10, 8, 8, 9, 9, 8, 10, -30, 9, 9], 'sumrewards': 50, 'aggr': {'avg': [5.0], 'min': [-30], 'max': [10]}}\n",
      "Generation 10, averages of top 10 agents:\n",
      "[[5.0], [2.9], [2.4], [0.6], [0.5], [0.5], [0.3], [-0.7], [-0.8], [-1.1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:08<00:00, 10.59agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 11, best results:\n",
      "{'rewards': [-10, 9, 9, 7, 8, 6, 10, 8, 8, 8], 'sumrewards': 63, 'aggr': {'avg': [6.3], 'min': [-10], 'max': [10]}}\n",
      "Generation 11, averages of top 10 agents:\n",
      "[[6.3], [5.2], [4.7], [4.7], [4.4], [3.9], [3.2], [3.1], [3.0], [2.7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:08<00:00, 11.95agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 12, best results:\n",
      "{'rewards': [9, 10, 10, 6, 8, 9, 9, 8, 8, 9], 'sumrewards': 86, 'aggr': {'avg': [8.6], 'min': [6], 'max': [10]}}\n",
      "Generation 12, averages of top 10 agents:\n",
      "[[8.6], [6.4], [6.4], [4.7], [4.4], [3.0], [2.9], [2.7], [2.4], [1.5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:07<00:00, 14.29agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 13, best results:\n",
      "{'rewards': [9, -12, 6, 9, 8, 7, 9, 6, -10, 10], 'sumrewards': 42, 'aggr': {'avg': [4.2], 'min': [-12], 'max': [10]}}\n",
      "Generation 13, averages of top 10 agents:\n",
      "[[4.2], [4.1], [3.6], [2.9], [2.8], [2.8], [2.7], [1.4], [1.4], [1.1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:07<00:00, 14.06agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 14, best results:\n",
      "{'rewards': [10, 10, 7, 9, 9, 9, 9, 7, 10, 8], 'sumrewards': 88, 'aggr': {'avg': [8.8], 'min': [7], 'max': [10]}}\n",
      "Generation 14, averages of top 10 agents:\n",
      "[[8.8], [5.1], [4.8], [4.6], [3.3], [3.2], [2.9], [2.9], [2.6], [2.6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:06<00:00, 15.33agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 15, best results:\n",
      "{'rewards': [10, 7, 9, 10, 10, 8, 8, 9, 8, 7], 'sumrewards': 86, 'aggr': {'avg': [8.6], 'min': [7], 'max': [10]}}\n",
      "Generation 15, averages of top 10 agents:\n",
      "[[8.6], [6.6], [6.5], [6.5], [6.3], [5.3], [5.1], [5.0], [5.0], [4.9]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    pop.evolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then change to 10x10 and run for 10 more generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:13<00:00,  7.18agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 16, best results:\n",
      "{'rewards': [-30, 7, -13, 6, 7, -30, 5, 6, 7, 9], 'sumrewards': -26, 'aggr': {'avg': [-2.6], 'min': [-30], 'max': [9]}}\n",
      "Generation 16, averages of top 10 agents:\n",
      "[[-2.6], [-2.7], [-3.0], [-6.0], [-7.0], [-7.4], [-9.3], [-10.1], [-10.5], [-11.3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:14<00:00,  7.39agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 17, best results:\n",
      "{'rewards': [4, -30, 3, 8, 7, -30, 4, -30, 4, 2], 'sumrewards': -58, 'aggr': {'avg': [-5.8], 'min': [-30], 'max': [8]}}\n",
      "Generation 17, averages of top 10 agents:\n",
      "[[-5.8], [-5.8], [-8.2], [-8.8], [-9.3], [-9.7], [-9.7], [-10.8], [-11.5], [-11.7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:13<00:00,  6.54agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 18, best results:\n",
      "{'rewards': [-30, 8, 3, 7, -3, -10, 10, 7, 6, -4], 'sumrewards': -6, 'aggr': {'avg': [-0.6], 'min': [-30], 'max': [10]}}\n",
      "Generation 18, averages of top 10 agents:\n",
      "[[-0.6], [-4.2], [-4.4], [-5.7], [-6.3], [-7.4], [-8.0], [-8.7], [-8.7], [-8.9]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:13<00:00,  7.69agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 19, best results:\n",
      "{'rewards': [9, 7, -10, 2, 8, 2, 6, 8, 3, -30], 'sumrewards': 5, 'aggr': {'avg': [0.5], 'min': [-30], 'max': [9]}}\n",
      "Generation 19, averages of top 10 agents:\n",
      "[[0.5], [-1.2], [-3.5], [-5.3], [-5.6], [-6.3], [-6.9], [-7.3], [-8.3], [-9.4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:12<00:00,  7.20agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 20, best results:\n",
      "{'rewards': [3, 8, 7, 6, 10, -15, 4, -1, 4, 6], 'sumrewards': 32, 'aggr': {'avg': [3.2], 'min': [-15], 'max': [10]}}\n",
      "Generation 20, averages of top 10 agents:\n",
      "[[3.2], [-4.1], [-4.2], [-4.5], [-4.6], [-4.8], [-5.1], [-5.2], [-5.5], [-5.7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:12<00:00,  8.27agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 21, best results:\n",
      "{'rewards': [-30, 5, 4, 8, -30, 10, 6, 0, 3, 2], 'sumrewards': -22, 'aggr': {'avg': [-2.2], 'min': [-30], 'max': [10]}}\n",
      "Generation 21, averages of top 10 agents:\n",
      "[[-2.2], [-2.3], [-5.3], [-5.7], [-5.9], [-6.0], [-6.1], [-6.2], [-6.3], [-6.5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:12<00:00,  9.12agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 22, best results:\n",
      "{'rewards': [4, 2, 4, 2, 6, 6, -1, -1, 8, 5], 'sumrewards': 35, 'aggr': {'avg': [3.5], 'min': [-1], 'max': [8]}}\n",
      "Generation 22, averages of top 10 agents:\n",
      "[[3.5], [1.2], [-0.3], [-2.2], [-3.7], [-3.7], [-4.0], [-4.4], [-5.7], [-6.2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:11<00:00,  8.55agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 23, best results:\n",
      "{'rewards': [4, -12, 10, 1, 7, -11, 2, -30, 9, 1], 'sumrewards': -19, 'aggr': {'avg': [-1.9], 'min': [-30], 'max': [10]}}\n",
      "Generation 23, averages of top 10 agents:\n",
      "[[-1.9], [-2.7], [-3.0], [-3.2], [-3.6], [-4.3], [-4.6], [-4.6], [-4.9], [-5.3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:11<00:00,  9.28agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 24, best results:\n",
      "{'rewards': [10, 5, 6, -30, 8, 5, 4, 10, 4, 2], 'sumrewards': 24, 'aggr': {'avg': [2.4], 'min': [-30], 'max': [10]}}\n",
      "Generation 24, averages of top 10 agents:\n",
      "[[2.4], [1.3], [1.1], [1.0], [-0.5], [-1.8], [-1.9], [-2.2], [-2.4], [-2.6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 100/100 [00:10<00:00,  9.53agents/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 25, best results:\n",
      "{'rewards': [6, 5, 7, 0, 2, -14, 7, 8, 6, 7], 'sumrewards': 34, 'aggr': {'avg': [3.4], 'min': [-14], 'max': [8]}}\n",
      "Generation 25, averages of top 10 agents:\n",
      "[[3.4], [3.1], [2.7], [2.2], [1.1], [-0.4], [-1.4], [-1.5], [-1.5], [-1.6]]\n"
     ]
    }
   ],
   "source": [
    "X,Y = 10, 10\n",
    "for i in range(10):\n",
    "    pop.evolve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfrl4] *",
   "language": "python",
   "name": "conda-env-tfrl4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
